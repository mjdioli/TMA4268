---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group XYZ"
author: "NN1, NN2 and NN3"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document
  #pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error=FALSE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
```

```{r,eval=FALSE,echo=TRUE}
install.packages("knitr") #probably already installed
install.packages("rmarkdown") #probably already installed
install.packages("bestglm")# for subset selection with categorical variables
install.packages("glmnet")# for lasso
install.packages("tree") #tree
install.packages("randomForest") #for random forest
install.packages("ElemStatLearn") #dataset in Problem 2
BiocManager::install(c("pheatmap")) #heatmap in Problem 2
```

```{r,eval=FALSE}
install.packages("ggplot2")
install.packages("GGally") # for ggpairs
install.packages("caret") #for confusion matrices
install.packages("pROC") #for ROC curves
install.packages("e1071") # for support vector machines
install.packages("nnet") # for feed forward neural networks
```

# Problem 1: Regression [6 points]

```{r}
all=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/diamond.dd")
dtrain=all$dtrain
dtest=all$dtest
print(all)
```

**Q1**: Would you choose `price` or `logprice` as response variable? Justify your choice. Next, plot your choice of response pairwise with `carat`, `logcarat`, `color`, `clarity` and `cut`. Comment.

```{r}
library(ggplot2)
# fitting a linear model with price as response
#lm(log(FEV) ~ Age + Htcm + Gender + Smoke, data=lungcap)
modelP = lm(price ~ carat+ logcarat+ as.factor(cut)+ as.factor(color)+ as.factor(clarity)+ depth+ table+ xx+ yy+ zz, data = dtrain)
  
# Plotting residuals residuls vs fitted
ggplot(modelP, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. Standardized residuals",
       subtitle = deparse(modelP$call))

# qq-plot of residuals
ggplot(modelP, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", 
       title = "Normal Q-Q", subtitle = deparse(modelP$call))

#Fitting a linear model with logprice as response
modelLP = lm(logprice ~ carat+ logcarat+ as.factor(cut)+ as.factor(color)+ as.factor(clarity)+ depth+ table+ xx+ yy+ zz, data = dtrain)
  
  
# Plotting residuals vs fitted
ggplot(modelLP, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. Standardized residuals",
       subtitle = deparse(modelLP$call))

# qq-plot of residuals
ggplot(modelLP, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", 
       title = "Normal Q-Q", subtitle = deparse(modelLP$call))


library(GGally)
#ggpairs(dtrain,lower = list(combo = wrap(ggally_facethist, binwidth = 2)))
pairs(dtrain, cex = 0.05)
#TODO: Plot kunne v√¶rt bedre

```

Use the local regression model $\texttt{logprice} = \beta_0 + \beta_1 \texttt{carat}+ \beta_2 \texttt{carat}^2$ weighted by the tricube kernel $K_{i0}$. 

Looking at the plots we prefer log(price) as our response variable [TODO:Explain why]. [TODO:Andreas, comment on the pairwise plot with the response.]

**Q2:** What is the predicted price of a diamond weighting 1 carat. Use the closest 20% of the observations.  
```{r}
fitLocal = loess(log(price) ~ carat + logcarat, data = dtrain, span = .2) #Uncertain about this local regression
'?'(loess)
#TODO fix plotting
plot(fitLocal, main = "Local Regression") 
```

**Q3:** What choice of $\beta_1$, $\beta_2$ and $K_{i0}$ would result in KNN-regression?  

Choosing $\beta_1$, $\beta_2$ and $K_{i0}$ such that we minimize $\sum_{i=1}^{n} K_{i0}(y_{i}-\beta_{0}-\beta_{1}x_{i}-\beta_{2}x_{i}^{2})^{2}$ #TODO[ADD FROM AND TO TO THE SUM] would result in a KNN-regression, also called a local regression.
```{r}
summary(fitLocal)
```


**Q4:** Describe how you can perform model selection in regression with AIC as criterion. 

We have that the $\text{AIC}= \frac{1}{\hat\sigma^2}(\text{RSS}+2d\hat\sigma^2)$. The AIC can be used to indirectly estimate the test error in a data poor situation. We can then select the model with the lowest AIC as our model, where AIC penalizes more complex models.

**Q5:** What are the main differences between using AIC for model selection and using cross-validation (with mean squared test error MSE)?  

Since AIC is an estimation of the test error, it is less accurate than CV which actually calculates the test error. However, CV requires us to split our data into a training set, a test, and a validation set, meaning that there is less data to train our model on. Additionally, CV makes fewer assumptions about the underlying model than the AIC, and can be used when the model's degrees of freedom are hard to ascertain.

**Q6:** See the code below for performing model selection with `bestglm()` based on AIC. What kind of contrast is used to represent `cut`, `color` and `clarity`? Write down the final best model and explain what you can interpret from the model. 
```{r}
bestglm(Xy=ds, IC="AIC")
```








**Q7:** Calculate and report the MSE of the test set using the best model (on the scale of `logprice`).  

```{r,eval=FALSE}
library(bestglm)
ds=as.data.frame(within(dtrain,{
  y=logprice    # setting reponse
  logprice=NULL # not include as covariate
  price=NULL    # not include as covariate
  carat=NULL    # not include as covariate
  }))
fit=bestglm(Xy=ds, IC="AIC")$BestModel
summary(fit)

dte=as.data.frame(within(dtest,{
  y=logprice    # setting reponse
  logprice=NULL # not include as covariate
  price=NULL    # not include as covariate
  carat=NULL    # not include as covariate
  }))

#Calculating MSE using the best model
pred = predict.lm(fit, dte)

mean((dte$y - pred)^2)

```
Using the best model we get a test MSE of 0.003600305.


**Q8:** Build a model matrix for the covariates `~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1`. What is the dimension of this matrix?  

```{r}
modelMatrix = 
```


**Q9:** Fit a lasso regression to the diamond data with `logprice` as the response and the model matrix given in Q8. How did you find the value to be used for the regularization parameter?  
```{r}

```

**Q10:** Calculate and report the MSE of the test set (on the scale of `logprice`).  
```{r}

```

**Q11:** A regression tree to model is built using a _greedy_ approach. What does that mean? Explain the strategy used for constructing a regression tree.  
A regression tree with recursive binary splitting (The one we were taught about in class and the one we will be describing here) divides the predictor space into two regions, $R_{1}$ and $R_{2}$, by making a decision rule for one of the predictors. Our goal is to find a predictor $x_{j}$ and a splitting point S such that $\sum_{i:x_{i}\in R_{1}(j,S)}(y_{i}-\hat y_{R_{1}})^2 + \sum_{i:x_{i}\in R_{2}(j,S)}(y_{i}-\hat y_{R_{2}})^2$ is minimized, where $\hat y_{R_{1}}$ and $\hat y_{R_{2}}$ are the mean responses for the training obsercations in $R_{1}(j,S)$ and $R_{2}(j,S)$ respectively. Our building method being greedy means that we at each stage only look at the optimal solution to the minimization problem for that instance and do not consider future instances.


**Q12:** Is a regression tree a suitable method to handle both numerical and categorical covariates? Elaborate.  
Decision trees can handle both categorical and numerical variables with little difference in performance. Every split is based on a feature, and if the feature is categorical, the split is done with the elements belonging to a particular class. Regression trees themselves have some major disadvantages which makes them less useful than other competing methods. Chief among this is their poor predictive performance due to high variance and that they're not very robust, as small changes in the data can cause large changes in the final estimated tree. The main advantage of trees, which is more pronounced for small trees, is the ease with which humans can understand and interpret them.

**Q13:** Fit a (full) regression tree to the diamond data with `logprice` as the response (and the same covariates as for c and d), and plot the result. Comment briefly on you findings.  

```{r}
library(tree)

tree = tree(logprice ~ logcarat+cut+clarity+color+depth+table+xx+yy+zz, dtrain)
summary(tree)
plot(tree, type = "proportional")
text(tree, pretty = 1)
```
We see that the regression tree only makes use of the length and width in mm of the diamond (xx and yy). This might be because length and width are highly correlated with other parts of the data.


**Q14:** Calculate and report the MSE of the test set (on the scale of `logprice`). 
```{r}
library(caret)
tree.pred = predict(tree, dtest)
mean((dte$y - tree.pred)^2)
```
The MSE of the test set is 0.01715548

**Q15:** Explain the motivation behind bagging, and how bagging differs from random forest? What is the role of bootstrapping? 
Bagging, also known as bootstrap aggregation, is a technique for dealing with the high variance of decision trees. Bagging is accomplished in the following manner:
  - We use bootstrapping to construct k independent data sets from our original data set. 
  - We then fit a decision tree to each data set.
  - We take the average of our fitted trees as our bagged tree. For regression trees we take the average of all the predictions,
    while for classification trees we record the class for a given observation for each of the k trees use the majority vote or       average posterior probabilities for each class as our final prediction.

This works because the variance of the mean is reduced by the number of instances we average over.If there is a strong predictor in the data set then the decision trees produced by each of the bootstrap samples will look very similar to eachother. This is not optimal, as the k trees we get will be highly correlated, which means the variance of the bagged tree will be $\frac{1-(1-k)\rho}{k-1}\sigma^2$ where $\rho$ is the correlation between the trees. Random forest reduces the variance by reducing the correlation between the trees. The procedure is the same as in bagging but with the difference that at each split we are only allowed to consider $m<p$ of the predictors with a new sample of m predictors taken at each split.

**Q16:** What are the parameter(s) to be set in random forest, and what are the rules for setting these?  

The only tuning parameter we have is the number of predictors to consider at each split, m. The new sample of m preictors should be chosen to be small for highly correlated predictors p. Typically we have that $m\approx \sqrt{p} \text{ for classification and } m = \frac{p}{3}$ for regression.

**Q17:** Boosting is a popular method. What is the main difference between random forest and boosting? 
#TODO: PRUNE THIS ANWSER AS IT IS OVER-DESCRIPTVE
Boosting is done in the following manner:
  - First build a decision tree with d splits.
  - Fit a decision tree to the residuals of the model.
  - Update the original tree with the residual tree multiplied with a weight.
  - Repeat until a stopping criterion is reached.

The main differences between boosting and random forest is that in random forest the decision trees are grown in paralell while in boosting they are grown sequentially.


**Q18:** Fit a random forest to the diamond data with `logprice` as the response (and the same covariates as before). Comment on your choice of parameter (as decribed in Q16).   

```{r}
library(randomForest)
rf = randomForest(logprice ~ logcarat+cut+clarity+color+depth+table+xx+yy+zz, dtrain, mtry = 3, ntree = 300, importance = TRUE)

```

We choose $m=3$ as the parameter for our random forrest since this is a regression model and $\frac{p}{3}=\frac{9}{3}=3$ according to the rule described in Q16.

**Q19:** Make a variable importance plot and comment on the plot. Calculate and report the MSE of the test set (on the scale of `logprice`).  

```{r}
varImpPlot(rf, pch = 20)
rf.pred = predict(rf, dtest)
mean((dte$y - rf.pred)^2)

```
The fact that our variable importance plots disagree so clearly indicates that there are some highly correlated features in the data.


The MSE of the test set is 0.002794626.


**Q20:** Finally, compare the results from c (subset selection), d (lasso), e (tree) and f (random forest): Which method has given the best performance on the test set and which method has given you the best insight into the relationship between the price and the covariates?



# Problem 2: Unsupervised learning [3 points]

**Q21:** What is the definition of a principal component score, and how is the score related to the eigenvectors of the matrix ${\hat {\bf R}}$. 


A principal component $q_{m}$ is defined as $q{m}=\sum_{m=1}^{M}\phi_{jm}Z_{j}$ where $\phi_{j}$ equals the column vector corresponding to the largest eigencalue of the covairance matrix ${\hat {\bf R}}$ of $Z$. Each entry in the principal component vector is a principal component score. PCA is sensitive to scales, so standardizing the data is necessary, which is why we use Z instead of X.

**Q22:** Explain what is given in the plot with title "First eigenvector". Why are there only $n=64$ eigenvectors and $n=64$ principal component scores? 


The design matrix X has dimension $64*6830$, meaning that it has only 64 eigenvectors. Since each principal component score is a linear combination of columns of X, and X has 64 columns, then there can only be 64 principal component scores.



**Q23:** How many principal components are needed to explain 80% of the total variance in ${\bf Z}$? Why is `sum(pca$sdev^2)=p`?
```{r}
#Computes variance and plots proportion of variance explained by each principal component
pr.var=pca$sdev^2
pve=pr.var/sum(pr.var)
plot(cumsum(pve), 
     xlab="Principal Component", 
     ylab="Cumulative Proportion of Variance Explained", 
     ylim=c(0,1),
     type='b')
abline(h=0.8)
abline(v=32)
```
32 principal components are needed to explain 80% of the total variance. sum(pca$ $sdev^2)=p because this sum gives the total variance of the data, which is also given by the trace of ${\hat {\bf R}}$. Since ${\hat {\bf R}}$ is a linear combination of the standardized matrix Z, it has only 1 on it's diagonal, and since ${\hat {\bf R}}$ is a square matrix, its trace is the sum of its diagonal elements, giving us p.



**Q24**: Study the PC1 vs PC2 plot, and comment on the groupings observed. What can you say about the placement of the `K262`, `MCF7` and `UNKNOWN` samples? Produce the same plot for two other pairs of PCs and comment on your observations.


```{r}
library(ElemStatLearn)
X=t(nci) #n times p matrix
table(rownames(X))
ngroups=length(table(rownames(X)))
cols=rainbow(ngroups)
cols[c(4,5,7,8,14)] = "black"
pch.vec = rep(4,14)
pch.vec[c(4,5,7,8,14)] = 15:19

colsvsnames=cbind(cols,sort(unique(rownames(X))))
colsamples=cols[match(rownames(X),colsvsnames[,2])] 
pchvsnames=cbind(pch.vec,sort(unique(rownames(X))))
pchsamples=pch.vec[match(rownames(X),pchvsnames[,2])]

Z=scale(X)

pca=prcomp(Z)
plot(pca$x[,1],pca$x[,2],xlab="PC1",ylab="PC2",pch=pchsamples,col=colsamples)
legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)

plot(1:dim(X)[2],pca$rotation[,1],type="l",xlab="genes",ylab="weight",main="First eigenvector")
```



**Q25:**: Explain what it means to use Euclidean distance and average linkage for hierarchical clustering. 
Both Euclidean distance and average linkage are measures of dissimilarity. However, Euclidean distance is only a measure of dissimilarity between pairs of observations, while average linkage is a measure of dissimilarity between pairs of groups of observations.


**Q26:**: Perform hierarchical clustering with Euclidean distance and average linkage on the scaled gene expression in `Z`. Observe where our samples labelled as K562, MCF7 and `UNKNOWN` are placed in the dendrogram. Which conclusions can you draw from this?

```{r}
hc.average=hclust(dist(Z), method="average")
plot(hc.average, main="Average Linkage", xlab="", sub="", cex=.6)
```

We can conclude that 'UNKNOWN' is in some way related to ovarian cancer, that K562 is highly related to leukemia (since it is both close in euclidean distance and the cluster K562 - Leukemia is close to Leukemia), and that MCF7 is related to breast cancer and slightly related to colon cancer.


**Q27:**: Study the R-code and plot below. Here we have performed hierarchical clustering based on thefirst 64 principal component instead of the gene expression data in `Z`. What is the difference between using all the gene expression data and using the first 64 principal components in the clustering? We have plotted the dendrogram together with a heatmap of the data. Explain what is shown in the heatmap. What is given on the horizontal axis, vertical axis, value in the pixel grid?

```{r}
library(pheatmap)
npcs=64 
'?'(pheatmap)
pheatmap(pca$x[,1:npcs],scale="none",cluster_col=FALSE,cluster_row=TRUE,clustering_distance_rows = "euclidean",clustering_method="average",fontsize_row=5,fontsize_col=5)
```
In the heatmap above, the horizontal axis gives the 64 principal components, the vertical axis gives the different cancer cells and their place in the dendrogram, and the value in the pixel grid gives... ##TODO FILL IN THIS





# Problem 3: Flying solo with diabetes data [6 points]

```{r}
flying=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/flying.dd")
ctrain=flying$ctrain
ctest=flying$ctest
pairs(ctrain, cex = 0.01)
print(flying)
#Digesting data with pca, random forest feature importance, correlation plots, normality and residual plots:

#Logistic regression techniques
fit = glm(diabetes ~ ., family = "binomial", data = ctrain)
summary(fit)
  
# Plotting residuals residuls vs fitted
ggplot(modelP, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. Standardized residuals",
       subtitle = deparse(modelP$call))

# qq-plot of residuals
ggplot(modelP, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", 
       title = "Normal Q-Q", subtitle = deparse(modelP$call))

#Random forest
'?'(randomForest)
rf_diabetes = randomForest(diabetes ~ ., ctrain, mtry = 3, ntree = 150, importance = TRUE)
varImpPlot(rf_diabetes, pch = 20)

#PCA
Q=scale(ctrain)

pca_diabetes=prcomp(Q)
plot(pca_diabetes$x[,1],pca_diabetes$x[,2],xlab="PC1",ylab="PC2",pch=pchsamples,col=colsamples)
#TODO:FIX LEGEND
legend("bottomright",legend = colsvsnames[,2],cex=0.55,col=cols,pch = pch.vec)

pca.var=pca_diabetes$sdev^2
pve_diabetes=pca.var/sum(pca.var)
plot(cumsum(pve_diabetes), 
     xlab="Principal Component", 
     ylab="Cumulative Proportion of Variance Explained", 
     ylim=c(0,1),
     type='b')

#Adding heatmaps and clustering
npcs=8 
'?'(pheatmap)
pheatmap(pca_diabetes$x[,1:npcs],scale="none",cluster_col=FALSE,cluster_row=TRUE,clustering_distance_rows = "euclidean",clustering_method="average",fontsize_row=2,fontsize_col=10)
#TODO Make heatmap more readable
```

Conclusion: We can conclude from our random forest and logistic regression that glu, ped, and bmi are the most important features.




**Q28:** Start by getting to know the _training data_, by producing summaries and plots. Write a few sentences about what you observe and include your top 3 informative plots and/or outputs.

**Q29:** Use different methods to analyse the data. In particular use 

* one method from Module 4: Classification
* one method from Module 8: Trees (and forests)
* one method from Module 9: Support vector machines and, finally
* one method from Module 11: Neural networks

For each method you 

* clearly write out the model and model assumptions for the method
* explain how any tuning parameters are chosen or model selection is performed
* report (any) insight into the interpretation of the fitted model
* evaluate the model using the test data, and report misclassifiation rate (cut-off 0.5 on probability) and plot ROC-curves and give the AUC (for method where class probabilities are given).

**Q30:** Conclude with choosing a winning method, and explain why you mean that this is the winning method.

